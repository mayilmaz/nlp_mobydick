{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tools for text processing\n",
    "\n",
    "What are the most frequent words in Herman Melville's novel, Moby Dick, and how often do they occur?\n",
    "\n",
    "In this notebook, we'll scrape the novel <em>Moby Dick</em> from the website <a href=\"https://www.gutenberg.org/\">Project Gutenberg</a> (which contains a large corpus of books) using the Python package <code>requests</code>. Then we'll extract words from this web data using <code>BeautifulSoup</code>. Finally, we'll analyze the distribution of words using the Natural Language ToolKit (<code>nltk</code>) and <code>Counter</code>.\n",
    "\n",
    "The Data Science pipeline we'll build in this notebook can be used to visualize the word frequency distributions of any novel that you can find on Project Gutenberg. The natural language processing tools used here apply to much of the world's data as it is unstructured data and includes a great amount of text.\n",
    "\n",
    "We will start by loading in the main Python packages we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requests, BeautifulSoup, nltk, and Counter\n",
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Request Moby Dick\n",
    "<p>To start analyzing Moby Dick, we need to get the contents of Moby Dick from <em>somewhere</em>. We are lucky that the text is freely available online at Project Gutenberg as an HTML file: <a href=\"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\">https://www.gutenberg.org/files/2701/2701-h/2701-h.htm</a> .</p>\n",
    "\n",
    "<p>To fetch the HTML file with Moby Dick we're going to use the <code>requests</code> package to make a <code>GET</code> request for the website, which means we're <em>getting</em> data from it. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n",
      "\r\n",
      "<!DOCTYPE html\r\n",
      "   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\r\n",
      "   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\r\n",
      "\r\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\r\n",
      "  <head>\r\n",
      "    <title>\r\n",
      "      Moby Dick; Or the Whale, by Herman Melville\r\n",
      "    </title>\r\n",
      "    <style type=\"text/css\" xml:space=\"preserve\">\r\n",
      "\r\n",
      "    body { background:#ffffff; color:black; margin-left:15%; margin-right:15%; text-align:justify }\r\n",
      "    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\r\n",
      "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\r\n",
      "    hr  { width: 50%; text-align: center;}\r\n",
      "    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\r\n",
      "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\r\n",
      "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\r\n",
      "    .toc       {\n"
     ]
    }
   ],
   "source": [
    "# Getting the Moby Dick HTML \n",
    "r = requests.get(\"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\")\n",
    "\n",
    "# Setting the correct text encoding of the HTML page\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "# Extracting the HTML from the request object\n",
    "html = r.text\n",
    "\n",
    "# Printing the first 1000 characters in html\n",
    "print(html[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the text from the HTML\n",
    "<p>Currently this HTML is not quite what we want. However, it does <em>contain</em> what we want: the text of <em>Moby Dick</em>. What we need to do now is <em>wrangle</em> this HTML to extract the text of the novel. For this we'll use the package <code>BeautifulSoup</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R 29. Enter Ahab; to Him, Stubb. \n",
      "\n",
      "\n",
      " CHAPTER 30. The Pipe. \n",
      "\n",
      "\n",
      " CHAPTER 31. Queen Mab. \n",
      "\n",
      "\n",
      " CHAPTER 32. Cetology. \n",
      "\n",
      "\n",
      " CHAPTER 33. The Specksnyder. \n",
      "\n",
      "\n",
      " CHAPTER 34. The Cabin-Table. \n",
      "\n",
      "\n",
      " CHAPTER 35. The Ma\n"
     ]
    }
   ],
   "source": [
    "# Creating a BeautifulSoup object from the HTML\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Getting the text out of the soup\n",
    "text = soup.get_text()\n",
    "\n",
    "# Printing out text between characters 3200 and 3400\n",
    "print(text[3200:3400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract the words\n",
    "<p>Now we have the text of the novel. There is some unwanted parts at the start and some unwanted parts at the end. It is possible to remove it, but for the sake of simplicity right now we will leave it as is.</p>\n",
    "<p>As we possess the text of interest, it's time to count how many times each word appears, and for this we'll use <code>nltk</code> â€“ the Natural Language Toolkit. We'll start by tokenizing the text, that is, remove everything that isn't a word (whitespace, punctuation, etc.) and then split the text into a list of words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Moby', 'Dick', 'Or', 'the', 'Whale', 'by', 'Herman', 'Melville']\n"
     ]
    }
   ],
   "source": [
    "# Creating a tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+,;:')\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "print(tokens[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make the words lowercase\n",
    "<p>We're proceeding with success. Note that in the above 'Or' has a capital 'O' and that in other places it may not, but both 'Or' and 'or' should be counted as the same word. For this reason, we should build a list of all words in <em>Moby Dick</em> in which all capital letters have been made lower case.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"
     ]
    }
   ],
   "source": [
    "# Create a list called words containing all tokens transformed to lower-case\n",
    "words = []\n",
    "for x in tokens:\n",
    "    words.append(x.lower())\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load in stop words\n",
    "<p>It is common practice to remove words that appear a lot in the English language such as 'the', 'of' and 'a' because they're not so interesting. Such words are known as <em>stop words</em>. The package <code>nltk</code> includes a good list of stop words in English that we can use.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ylmza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Getting the English stop words from nltk\n",
    "nltk.download('stopwords')\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "print(sw[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Remove stop words in Moby Dick\n",
    "<p>We will now create a new list with all <code>words</code> in Moby Dick, except those that are stop words (that is, those words listed in <code>sw</code>).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moby', 'dick', 'whale', 'herman', 'melville']\n"
     ]
    }
   ],
   "source": [
    "# Create a list words_ns containing all words that are in words but not in sw\n",
    "words_ns = [w for w in words if w not in sw]\n",
    "\n",
    "# Printing the first 5 words_ns to check that stop words are gone\n",
    "print(words_ns[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Original question\n",
    "<p>Let's have a look at our original question:</p>\n",
    "<blockquote>\n",
    "  <p>What are the most frequent words in Moby Dick and how often do they occur?</p>\n",
    "</blockquote>\n",
    "<p>We are now ready to answer this question. Let's answer it using the <code>Counter</code> class we imported earlier.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whale', 1245), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Counter object from our processed list of words\n",
    "count = Counter(words_ns)\n",
    "\n",
    "# Store 10 most common words and their counts as top_ten\n",
    "top_ten = count.most_common(10)\n",
    "\n",
    "# Print the top ten words and their counts\n",
    "print(top_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. The most common word\n",
    "<p>Using our variable <code>top_ten</code>, now we have an answer to our original question.</p>\n",
    "\n",
    "<p><em>Not surprisingly</em> the most common word in Moby Dick is <em>'whale'</em>.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
